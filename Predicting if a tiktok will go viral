{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9047914,"sourceType":"datasetVersion","datasetId":5455179}],"dockerImageVersionId":30761,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/andreaszurhaar/predicting-if-a-tiktok-will-go-viral?scriptVersionId=196740334\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# Predicting if a tiktok will go viral\n\nThis notebook demonstrates how to build a natural language processing (NLP) random forest model to predict whether a TikTok video will go viral. Unlike traditional models, this approach mimics a democratic process. Multiple models are used, each making its own predictionâ€”either \"yes\" (the video will go viral, represented by 1) or \"no\" (the video will not go viral, represented by 0). These individual predictions are then averaged and rounded to arrive at a final decision of either yes or no.\n\nThis method offers two key advantages. First, because each sub-model is trained on a different portion of the dataset, it allows for training on very large datasets, even on a laptop. Second, the likelihood of overfitting is reduced since the model operates as an ensemble, combining multiple predictions rather than relying on a single model. ","metadata":{}},{"cell_type":"markdown","source":"## Step 1: Viewing the data\n\nThis gives an insight to the type of data we have for training. The model used later will only use the 'video_view_count' as the target variable and the 'video_transcription_text' to train.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\n# Load the CSV file into a Pandas DataFrame\ndf = pd.read_csv('/kaggle/input/dataset-from-tiktok/tiktok_dataset.csv')\n\n# Display the first few rows of the DataFrame to verify it was loaded correctly\nprint(df.head())\n\n","metadata":{"execution":{"iopub.status.busy":"2024-09-15T11:27:36.776899Z","iopub.execute_input":"2024-09-15T11:27:36.777518Z","iopub.status.idle":"2024-09-15T11:27:36.885787Z","shell.execute_reply.started":"2024-09-15T11:27:36.777465Z","shell.execute_reply":"2024-09-15T11:27:36.884417Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Step 2: Analyse Word Frequencies\n\nThe table below shows the most common words in the data and how frequently they appear. Analyzing word frequencies is important because it can help identify potential bias in your NLP model if certain words appear too frequently, which could influence the model's predictions. ","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport re\nfrom collections import Counter\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nimport nltk\nimport matplotlib.pyplot as plt\n\n# Load the CSV file into a Pandas DataFrame\ndf = pd.read_csv('/kaggle/input/dataset-from-tiktok/tiktok_dataset.csv')\n\n# Define a function for basic text cleaning\ndef clean_text(text):\n    if isinstance(text, str):\n        # Convert text to lowercase\n        text = text.lower()\n        # Remove URLs\n        text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n        # Remove user mentions and hashtags\n        text = re.sub(r'@\\w+|#\\w+', '', text)\n        # Remove special characters, numbers, and punctuation\n        text = re.sub(r'\\d+', '', text)\n        text = re.sub(r'[^\\w\\s]', '', text)\n        # Remove extra spaces\n        text = re.sub(r'\\s+', ' ', text).strip()\n    else:\n        text = \"\"  # Return an empty string if the input is not a string\n    return text\n\n# Apply the cleaning function to the 'video_transcription_text' column\ndf['cleaned_text'] = df['video_transcription_text'].apply(clean_text)\n\n# Tokenize the cleaned text\ndf['tokens'] = df['cleaned_text'].apply(word_tokenize)\n\n# Remove stop words from the tokens\nstop_words = set(stopwords.words('english'))\ndf['tokens'] = df['tokens'].apply(lambda tokens: [word for word in tokens if word not in stop_words])\n\n# Flatten the list of tokens and count word frequencies\nall_words = [word for tokens in df['tokens'] for word in tokens]\nword_freq = Counter(all_words)\n\n# Find the 20 most common tokens\nmost_common_tokens = word_freq.most_common(20)\n\n# Create a bar graph to display the most common bigrams\ntoken_labels, token_counts = zip(*most_common_tokens)\n\nplt.figure(figsize=(10, 6))\nplt.barh(token_labels, token_counts, color='skyblue')\nplt.xlabel('Frequency')\nplt.ylabel('Tokens')\nplt.title('Top 20 Most Common Tokens')\nplt.gca().invert_yaxis()  # Invert y-axis so the highest count appears at the top\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-09-15T11:27:36.888213Z","iopub.execute_input":"2024-09-15T11:27:36.888636Z","iopub.status.idle":"2024-09-15T11:27:44.275771Z","shell.execute_reply.started":"2024-09-15T11:27:36.888593Z","shell.execute_reply":"2024-09-15T11:27:44.274267Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Step 3: Analyse Most Common Bigrams\n\nBelow is a list of the most common bigrams in the data. Bigrams are pairs of words that appear together in the text. Two models will be trained: one using individual words (unigrams) and the other using bigrams. ","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport re\nimport nltk\nfrom nltk import word_tokenize, ngrams\nfrom nltk.corpus import stopwords\nfrom collections import Counter\nimport matplotlib.pyplot as plt\nnltk.download('stopwords')\n\n# Load the CSV file into a Pandas DataFrame\ndf = pd.read_csv('/kaggle/input/dataset-from-tiktok/tiktok_dataset.csv')\n\n# Define a function for basic text cleaning\ndef clean_text(text):\n    if isinstance(text, str):\n        # Convert text to lowercase\n        text = text.lower()\n        # Remove URLs\n        text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n        # Remove user mentions and hashtags\n        text = re.sub(r'@\\w+|#\\w+', '', text)\n        # Remove special characters, numbers, and punctuation\n        text = re.sub(r'\\d+', '', text)\n        text = re.sub(r'[^\\w\\s]', '', text)\n        # Remove extra spaces\n        text = re.sub(r'\\s+', ' ', text).strip()\n    else:\n        text = \"\"  # Return an empty string if the input is not a string\n    return text\n\n# Apply the cleaning function to the 'video_transcription_text' column\ndf['cleaned_text'] = df['video_transcription_text'].apply(clean_text)\n\n# Tokenize the cleaned text\ndf['tokens'] = df['cleaned_text'].apply(word_tokenize)\n\n# Remove stop words from the tokens\nstop_words = set(stopwords.words('english'))\ndf['tokens'] = df['tokens'].apply(lambda tokens: [word for word in tokens if word not in stop_words])\n\n# Generate bigrams (groups of two words)\ndf['bigrams'] = df['tokens'].apply(lambda tokens: list(ngrams(tokens, 2)) if len(tokens) > 1 else [])\n\n# Flatten the list of bigrams and count frequencies\nall_bigrams = [' '.join(bigram) for bigram_list in df['bigrams'] for bigram in bigram_list]\nbigram_freq = Counter(all_bigrams)\n\n# Find the 20 most common bigrams\nmost_common_bigrams = bigram_freq.most_common(20)\n\n# Create a bar graph to display the most common bigrams\nbigram_labels, bigram_counts = zip(*most_common_bigrams)\n\nplt.figure(figsize=(10, 6))\nplt.barh(bigram_labels, bigram_counts, color='skyblue')\nplt.xlabel('Frequency')\nplt.ylabel('Bigrams')\nplt.title('Top 20 Most Common Bigrams')\nplt.gca().invert_yaxis()  # Invert y-axis so the highest count appears at the top\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-09-15T11:34:15.099862Z","iopub.execute_input":"2024-09-15T11:34:15.101576Z","iopub.status.idle":"2024-09-15T11:34:21.468652Z","shell.execute_reply.started":"2024-09-15T11:34:15.10151Z","shell.execute_reply":"2024-09-15T11:34:21.46683Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Step 4: Analyse Most Common Phrases (trigrams)\n\nThis step will not be needed for the model that is built later, however the model could be modified to train on trigrams instead of bigrams and single words. I kept this section because it may be interesting for some to see what are the most common descriptors of tiktok videos. ","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport re\nfrom collections import Counter\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.util import ngrams\nimport nltk\n\n# Load the CSV file into a Pandas DataFrame\ndf = pd.read_csv('/kaggle/input/dataset-from-tiktok/tiktok_dataset.csv')\n\n# Define a function for basic text cleaning\ndef clean_text(text):\n    if isinstance(text, str):\n        # Convert text to lowercase\n        text = text.lower()\n        # Remove URLs\n        text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n        # Remove user mentions and hashtags\n        text = re.sub(r'@\\w+|#\\w+', '', text)\n        # Remove special characters, numbers, and punctuation\n        text = re.sub(r'\\d+', '', text)\n        text = re.sub(r'[^\\w\\s]', '', text)\n        # Remove extra spaces\n        text = re.sub(r'\\s+', ' ', text).strip()\n    else:\n        text = \"\"  # Return an empty string if the input is not a string\n    return text\n\n# Apply the cleaning function to the 'video_transcription_text' column\ndf['cleaned_text'] = df['video_transcription_text'].apply(clean_text)\n\n# Tokenize the cleaned text\ndf['tokens'] = df['cleaned_text'].apply(word_tokenize)\n\n# Remove stop words from the tokens\nstop_words = set(stopwords.words('english'))\ndf['tokens'] = df['tokens'].apply(lambda tokens: [word for word in tokens if word not in stop_words])\n\n# Generate trigrams (groups of three words)\ndf['trigrams'] = df['tokens'].apply(lambda tokens: list(ngrams(tokens, 3)) if len(tokens) > 2 else [])\n\n\n# Flatten the list of trigrams and count frequencies\nall_trigrams = [' '.join(trigram) for trigrams in df['trigrams'] for trigram in trigrams]\ntrigram_freq = Counter(all_trigrams)\n\n# Find the 20 most common bigrams\nmost_common_trigrams = trigram_freq.most_common(20)\n\n# Create a bar graph to display the most common bigrams\ntrigram_labels, trigram_counts = zip(*most_common_trigrams)\n\nplt.figure(figsize=(10, 6))\nplt.barh(trigram_labels, trigram_counts, color='skyblue')\nplt.xlabel('Frequency')\nplt.ylabel('Trigrams')\nplt.title('Top 20 Most Common Trigrams')\nplt.gca().invert_yaxis()  # Invert y-axis so the highest count appears at the top\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-09-15T11:35:25.460898Z","iopub.execute_input":"2024-09-15T11:35:25.461461Z","iopub.status.idle":"2024-09-15T11:35:31.34752Z","shell.execute_reply.started":"2024-09-15T11:35:25.461413Z","shell.execute_reply":"2024-09-15T11:35:31.345845Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Step 5: Set Up Target Variable to Predict and Determine What View Count 'is viral'\n\nThis step converts the 'video_view_count' variable into a binary column called 'is_viral'. A video is classified as viral if its view count exceeds the threshold defined by 'set_virality'. The 'is_viral' column will serve as the target variable that the models will aim to predict. ","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport re\nimport numpy as np\n\n\n# Set virality count\nset_virality = 100000\n# Load the CSV file into a Pandas DataFrame\ndf = pd.read_csv('/kaggle/input/dataset-from-tiktok/tiktok_dataset.csv')\n\n# Sample target variable (assume you have a binary target column 'is_viral')\ndf['is_viral'] = (df['video_view_count'] > set_virality).astype(int)\n\n# Drop rows where the target variable 'is_viral' is NaN\ndf = df.dropna(subset=['is_viral'])","metadata":{"execution":{"iopub.status.busy":"2024-09-15T11:35:48.758926Z","iopub.execute_input":"2024-09-15T11:35:48.759466Z","iopub.status.idle":"2024-09-15T11:35:48.866734Z","shell.execute_reply.started":"2024-09-15T11:35:48.759419Z","shell.execute_reply":"2024-09-15T11:35:48.865438Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Step 6: Clean the Text\nSince most datasets are raw and unfilitered with words and punctuation that only harms model training, it is important to clean the data. The nltk library has a built in function to filter out stopwords such as 'it, then, as, etc' so the model does not train with redundant words. ","metadata":{}},{"cell_type":"code","source":"import nltk\nfrom nltk.corpus import stopwords\n\n# Define a function for basic text cleaning\n# Download the stopwords corpus if you haven't already\nnltk.download('stopwords')\n\ndef clean_text(text):\n    if isinstance(text, str):\n        # Convert text to lowercase\n        text = text.lower()\n        \n        # Remove URLs\n        text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n        \n        # Remove mentions and hashtags\n        text = re.sub(r'@\\w+|#\\w+', '', text)\n        \n        # Remove digits\n        text = re.sub(r'\\d+', '', text)\n        \n        # Remove punctuation\n        text = re.sub(r'[^\\w\\s]', '', text)\n        \n        # Remove extra spaces\n        text = re.sub(r'\\s+', ' ', text).strip()\n        \n        # Remove stopwords\n        stop_words = set(stopwords.words('english'))\n        words = text.split()\n        words = [word for word in words if word not in stop_words]\n        \n        # Remove consecutive duplicate words\n        cleaned_text = []\n        for i in range(len(words)):\n            if i == 0 or words[i] != words[i-1]:  # Keep the word if it's not the same as the previous word\n                cleaned_text.append(words[i])\n        \n        text = ' '.join(cleaned_text)\n    else:\n        text = \"\"  # Return an empty string if the input is not a string\n    \n    return text","metadata":{"execution":{"iopub.status.busy":"2024-09-15T11:35:53.940201Z","iopub.execute_input":"2024-09-15T11:35:53.940694Z","iopub.status.idle":"2024-09-15T11:35:53.956414Z","shell.execute_reply.started":"2024-09-15T11:35:53.94065Z","shell.execute_reply":"2024-09-15T11:35:53.954755Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Step 7: Train the Token Random Forest Model\nThis first model is trained using vectorized tokens, which are the same as those described in step 2. The data is split into several parts, with each part assigned to a different sub-model. Importantly, the number of sub-models will always match the number of data parts. Each sub-model trains on the portion of data it receives. The final prediction is determined by averaging the predictions from all the sub-models and rounding the result to ensure it is either 1 or 0. A prediction of 1 means the model expects the TikTok to go viral, while 0 indicates the prediction that the TikTok will not go viral. ","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Define a custom function to generate single-word tokens\ndef generate_tokens(text):\n    # Tokenize the cleaned text (split by space to get single words)\n    words = text.split()\n    return words\n\n# Apply the cleaning function to the 'video_transcription_text' column\ndf['cleaned_text'] = df['video_transcription_text'].apply(clean_text)\n\n# Generate single-word tokens manually\ndf['tokens'] = df['cleaned_text'].apply(generate_tokens)\n\n# Join tokens into strings (necessary for vectorization)\ndf['tokens_str'] = df['tokens'].apply(lambda words: ' '.join(words))\n\n# Use the 'tokens_str' column which now contains manually created tokens\nvectorizer = CountVectorizer()  # No special token pattern needed for single words\n\n# Vectorize the tokens_str column\nX = vectorizer.fit_transform(df['tokens_str'])\n\n# Define the target variable\ny = df['is_viral']\n\n# Split the data into training and testing sets\nX_train_full, X_test, y_train_full, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Determine the number of parts to split the training data into\nn_parts = 100  # This can be adjusted\nsubset_size = X_train_full.shape[0] // n_parts  # Use shape[0] to get the number of rows\n\n# List to hold individual model predictions\nmodel_predictions = []\nmodels_single = []\nfor i in range(n_parts):\n    # Select the subset of training data\n    start_index = i * subset_size\n    if i == n_parts - 1:  # Last subset may include the remainder\n        end_index = X_train_full.shape[0]\n    else:\n        end_index = (i + 1) * subset_size\n\n    X_train_subset = X_train_full[start_index:end_index]\n    y_train_subset = y_train_full[start_index:end_index]\n\n    # Train a model on this subset\n    model = RandomForestClassifier(random_state=42)\n    models_single.append(model)\n    model.fit(X_train_subset, y_train_subset)\n\n    # Predict on the test set and store the predictions\n    y_pred = model.predict_proba(X_test)[:, 1]  # Get probability of the positive class\n    model_predictions.append(y_pred)\n\n# Average the predictions from all models\nfinal_predictions = np.mean(model_predictions, axis=0)\n\n# Convert probabilities to binary predictions\nfinal_predictions_binary = (final_predictions > 0.5).astype(int)\n\n# Evaluate the model\nfrom sklearn.metrics import accuracy_score\naccuracy = accuracy_score(y_test, final_predictions_binary)\nprint(f\"Ensemble Model Accuracy: {accuracy:.2f}\")","metadata":{"execution":{"iopub.status.busy":"2024-09-15T11:36:11.766097Z","iopub.execute_input":"2024-09-15T11:36:11.767394Z","iopub.status.idle":"2024-09-15T11:36:49.060542Z","shell.execute_reply.started":"2024-09-15T11:36:11.767342Z","shell.execute_reply":"2024-09-15T11:36:49.058924Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Step 8: Train Bigram Random Forest Model\n\nNow we will use the exact same methods as with the token model, but instead use bigrams to train instead of tokens.","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nimport numpy as np\n\n# Define a custom function to generate bigrams manually\ndef generate_bigrams(text):\n    # Tokenize the cleaned text (split by space)\n    words = text.split()\n    \n    # Create bigrams manually (consecutive word pairs)\n    bigrams = [(words[i], words[i+1]) for i in range(len(words)-1)]\n    \n    # Filter out bigrams where both words are identical\n    bigrams = [' '.join(bigram) for bigram in bigrams if bigram[0] != bigram[1]]\n    \n    return bigrams\n\n# Apply the cleaning function to the 'video_transcription_text' column\ndf['cleaned_text'] = df['video_transcription_text'].apply(clean_text)\n\n# Generate bigrams manually\ndf['bigrams'] = df['cleaned_text'].apply(generate_bigrams)\n\n# Join bigrams into strings (necessary for vectorization)\ndf['bigrams_str'] = df['bigrams'].apply(lambda bigrams: ' '.join(bigrams))\n\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n# Function to return bigrams as a list, so CountVectorizer treats them as pre-tokenized\ndef identity_tokenizer(text):\n    return text\n\n# Use the 'bigrams' column which already contains the manually created bigrams\nvectorizer_bigram = CountVectorizer(\n    tokenizer=identity_tokenizer,  # Use a custom tokenizer that does nothing\n    preprocessor=lambda x: x,  # Preprocessor that does nothing\n    token_pattern=None  # Disable default token pattern, as we provide tokens directly\n)\n\n# Pass the pre-tokenized bigrams list directly into the vectorizer\nX = vectorizer_bigram.fit_transform(df['bigrams'])\n\n# Define the target variable\ny = df['is_viral']\n\n# Split the data into training and testing sets\nX_train_full, X_test, y_train_full, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Determine the number of parts to split the training data into\nn_parts = 100  # This can be adjusted\nsubset_size = X_train_full.shape[0] // n_parts  # Use shape[0] to get the number of rows\n\n# List to hold individual model predictions\nmodel_predictions = []\nmodels = []\nfor i in range(n_parts):\n    # Select the subset of training data\n    start_index = i * subset_size\n    if i == n_parts - 1:  # Last subset may include the remainder\n        end_index = X_train_full.shape[0]\n    else:\n        end_index = (i + 1) * subset_size\n\n    X_train_subset = X_train_full[start_index:end_index]\n    y_train_subset = y_train_full[start_index:end_index]\n\n    # Train a model on this subset\n    model = RandomForestClassifier(random_state=42)\n    models.append(model)\n    model.fit(X_train_subset, y_train_subset)\n\n    # Predict on the test set and store the predictions\n    y_pred = model.predict_proba(X_test)[:, 1]  # Get probability of the positive class\n    model_predictions.append(y_pred)\n\n# Average the predictions from all models\nfinal_predictions = np.mean(model_predictions, axis=0)\n\n# Convert probabilities to binary predictions\nfinal_predictions_binary = (final_predictions > 0.5).astype(int)\n\n# Evaluate the model\nfrom sklearn.metrics import accuracy_score\naccuracy = accuracy_score(y_test, final_predictions_binary)\nprint(f\"Ensemble Model Accuracy: {accuracy:.2f}\")","metadata":{"execution":{"iopub.status.busy":"2024-09-15T11:37:17.087407Z","iopub.execute_input":"2024-09-15T11:37:17.088165Z","iopub.status.idle":"2024-09-15T11:38:10.101353Z","shell.execute_reply.started":"2024-09-15T11:37:17.088093Z","shell.execute_reply":"2024-09-15T11:38:10.09984Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Step 9: Asses Each Models Accuracy \nFor the token model the accuracy is equal to __0.93__ \\\nFor the bigram model the accuracy is equal to __0.68__ \\\n\\\nAt first glance, the token model may appear superior based on these accuracy scores. However, it's important to take a closer look to ensure that nothing is compromising the model's integrity. One way to investigate further is by examining the most influential tokens and bigrams used in predictions across all sub-models. We will explore this in the next two steps. ","metadata":{}},{"cell_type":"markdown","source":"## Step 10: Graph Important Features for Token Model\n\nThis function takes a list of trained random forest models and a fitted vectorizer\n    (either CountVectorizer or TfidfVectorizer), and plots the top N most important \n    token features across all models based on the aggregated feature importances.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\n\ndef plot_most_important_tokens_across_models(models, vectorizer, top_n=10):\n    \"\"\"\n    :param models: List of trained random forest models (either classifiers or regressors)\n    :param vectorizer: A fitted CountVectorizer or TfidfVectorizer used to extract tokens\n    :param top_n: Number of top important features to plot (default is 10)\n    \"\"\"\n    # Get the token feature names from the vectorizer\n    token_features = vectorizer.get_feature_names_out()\n\n    # Initialize an array to hold the total importances for each feature\n    total_feature_importances = np.zeros(len(token_features))\n\n    # Sum feature importances across all models\n    for model in models:\n        total_feature_importances += model.feature_importances_\n\n    # Get the indices of the top N important features across all models\n    top_indices = np.argsort(total_feature_importances)[-top_n:][::-1]\n    \n    # Get the corresponding feature names and importances\n    top_features = [token_features[idx] for idx in top_indices]\n    top_importances = total_feature_importances[top_indices]\n\n    # Plot the top N important features\n    plt.figure(figsize=(10, 6))\n    plt.barh(top_features, top_importances, color='skyblue')\n    plt.xlabel('Importance Score')\n    plt.title(f'Top {top_n} Most Important Tokens Across All Models')\n    plt.gca().invert_yaxis()  # To display the largest bar at the top\n    plt.show()\n\n# Example usage\nplot_most_important_tokens_across_models(models_single, vectorizer)\n\n\n\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-09-15T11:38:21.558402Z","iopub.execute_input":"2024-09-15T11:38:21.558985Z","iopub.status.idle":"2024-09-15T11:38:22.511453Z","shell.execute_reply.started":"2024-09-15T11:38:21.558929Z","shell.execute_reply":"2024-09-15T11:38:22.509934Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Step 11: Graph the Most Important Features of the Bigram Model\nThis function takes a list of trained random forest models and a fitted vectorizer\n    (either CountVectorizer or TfidfVectorizer), and plots the top N most important \n    bigram features across all models based on the aggregated feature importances.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.feature_extraction.text import CountVectorizer\n\ndef plot_most_important_bigrams_across_models(models, vectorizer, top_n=10):\n    \"\"\"\n    This function takes a list of trained random forest models and a fitted vectorizer\n    (either CountVectorizer or TfidfVectorizer), and plots the top N most important \n    bigram features across all models based on the aggregated feature importances.\n\n    :param models: List of trained random forest models (either classifiers or regressors)\n    :param vectorizer: A fitted CountVectorizer or TfidfVectorizer used to extract bigrams\n    :param top_n: Number of top important features to plot (default is 10)\n    \"\"\"\n    # Get the bigram feature names from the vectorizer\n    bigram_features = vectorizer.get_feature_names_out()\n\n    # Initialize an array to hold the total importances for each feature\n    total_feature_importances = np.zeros(len(bigram_features))\n\n    # Sum feature importances across all models\n    for model in models:\n        total_feature_importances += model.feature_importances_\n\n    # Get the indices of the top N important features across all models\n    top_indices = np.argsort(total_feature_importances)[-top_n:][::-1]\n    \n    # Get the corresponding feature names and importances\n    top_features = [bigram_features[idx] for idx in top_indices]\n    top_importances = total_feature_importances[top_indices]\n\n    # Plot the top N important bigrams\n    plt.figure(figsize=(10, 6))\n    plt.barh(top_features, top_importances, color='skyblue')\n    plt.xlabel('Importance Score')\n    plt.title(f'Top {top_n} Most Important Bigrams Across All Models')\n    plt.gca().invert_yaxis()  # To display the largest bar at the top\n    plt.show()\n\n# Example usage with bigrams in the vectorizer\nplot_most_important_bigrams_across_models(models, vectorizer_bigram)\n","metadata":{"execution":{"iopub.status.busy":"2024-09-15T11:38:33.602273Z","iopub.execute_input":"2024-09-15T11:38:33.602748Z","iopub.status.idle":"2024-09-15T11:38:35.01549Z","shell.execute_reply.started":"2024-09-15T11:38:33.602705Z","shell.execute_reply":"2024-09-15T11:38:35.014235Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Conclusion\nThe findings from step 10, which highlight the most important tokens for predicting virality, reveal an overlap with the most common tokens identified in the original data (step 2). This suggests that the model may have a bias, as it likely recognized that these frequently occurring tokens are more often found in the descriptions of viral TikToks. In other words, the model may be favoring these features simply because of their abundance. However, in the bigram model, there is less overlap with the most common tokens, and with its accuracy around 70%, the risk of bias is lower.\n\nDespite the inherent bias in the data, there are still valuable insights. For instance, the frequent appearance of words like \"media,\" \"claim,\" and \"news\" suggests that a significant portion of TikTok's audience is using the platform to receive news, or at least that these topics have naturally gained popularity. It indicates that users are interested in learning about current events and hearing claims from experts or celebrities. ","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}